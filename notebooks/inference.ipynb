{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "sys.path.insert(0, \"../src\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as tvt\n",
    "\n",
    "from config_manager.manager import Params\n",
    "import model.layers as layer\n",
    "from model.unet import UNet\n",
    "from utils.utils import read_json\n",
    "\n",
    "\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"data_path\": \"../output/dataset_20230117-152714\",\n",
    "    \"save_path\": \"../output/experiment\",\n",
    "    \"height\": 720,\n",
    "    \"width\": 1280,\n",
    "    \"resize\": 224,\n",
    "    \"scale\": 2.0,\n",
    "    \"filters\": [32, 64, 128, 256],\n",
    "    \"kernels\": [5, 3, 3, 3],\n",
    "    \"style_nodes\": [\"encoder.0.activation\", \"encoder.1.activation\", \"encoder.2.activation\"],\n",
    "    \"output_layer\": \"out_conv.activation\",\n",
    "}\n",
    "\n",
    "params = Params(param_dict)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = glob.glob(params.data_path + \"/**/*.png\", recursive=True)\n",
    "img_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path: str, params: Params) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "    \"\"\"Load image and transform to get a tensor for inference\"\"\"\n",
    "    img = np.asarray(Image.open(img_path))\n",
    "    img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "    result = ((img_tensor - img.min()) / (img.max() - img.min())).unsqueeze(0)\n",
    "\n",
    "    result = tvt.functional.affine(result, shear=0.0, scale=params.scale, translate=(0, 0), angle=0.0)\n",
    "    rsz = tvt.Resize((params.resize, params.resize))\n",
    "    result = rsz(result)\n",
    "\n",
    "    return img, result.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np, img_tensor = load_img(img_list[0], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_np.max())\n",
    "print(img_np.min())\n",
    "print(img_np.dtype)\n",
    "print()\n",
    "print(img_tensor.max())\n",
    "print(img_tensor.min())\n",
    "print(img_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_np, cmap=\"gray\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_tensor.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = UNet(params.filters, params.kernels)\n",
    "NODES = params.style_nodes + [params.output_layer]\n",
    "net = layer.ReconstructionModel(base_model, nodes=NODES)\n",
    "net.load_state_dict(\n",
    "    torch.load(os.path.join(params.save_path, \"best_net.pt\"), map_location=torch.device('cpu'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    pred = net(img_tensor)\n",
    "out = pred[params.output_layer]\n",
    "print(out.max())\n",
    "print(out.min())\n",
    "print(out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_prediction(img: torch.Tensor, max_: int, min_: int, params: Params) -> np.ndarray:\n",
    "    \"\"\"Unnormalize prediction to get original scale\"\"\"\n",
    "    tmp = img.clip(0.0, 1.0)\n",
    "    rsz = tvt.Resize((params.height, params.width))\n",
    "    result = rsz(tmp)\n",
    "    result = tvt.functional.affine(\n",
    "        result, shear=0.0, scale=1/params.scale, translate=(0, 0), angle=0.0\n",
    "    )\n",
    "    result = (result * (max_ - min_)) + min_\n",
    "    return result.squeeze().numpy().astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = unnormalize_prediction(out, img_np.max(), img_np.min(), params)\n",
    "print(res.max())\n",
    "print(res.min())\n",
    "print(res.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res, cmap=\"gray\")\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(params.data_path + \"/**/*.json\", recursive=True)\n",
    "\n",
    "intrinsic = o3d.camera.PinholeCameraIntrinsic(\n",
    "    **read_json(list(filter(lambda x: \"intrinsic\" in x, json_files))[0])\n",
    ")\n",
    "extrinsic = read_json(list(filter(lambda x: \"extrinsic\" in x, json_files))[0])[\"extrinsics\"]\n",
    "scale = read_json(list(filter(lambda x: \"scale\" in x, json_files))[0])[\"depth_scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud.create_from_depth_image(\n",
    "    o3d.geometry.Image(res),\n",
    "    #o3d.geometry.Image(img_np.astype(\"uint16\")),\n",
    "    intrinsic,\n",
    "    extrinsic,\n",
    "    depth_scale=scale,\n",
    "    depth_trunc=4000.0\n",
    ")\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv3d-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2f71e6011faa0109c6544a926ac70c80d3ea1a0e18aed42a92f791a760d489a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
